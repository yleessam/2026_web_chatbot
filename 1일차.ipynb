{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8269a472",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "619b171f-6a8b-45a5-a484-0d3163de28eb"
    }
   },
   "source": [
    "# LangChain으로 만드는 웹 기반 챗봇 실습  \n",
    "\n",
    "##  1. 과정 소개 및 개요\n",
    "- **학습 목표**\n",
    "  - AI 챗봇의 기본 개념과 발전 과정을 이해한다.\n",
    "  - 규칙 기반 챗봇과 LLM 기반 챗봇을 비교할 수 있다.\n",
    "  - LangChain, Streamlit, FAISS 등 주요 기술의 역할과 장점을 이해한다.\n",
    "  - LCEL을 사용하여 확장성 있는 체인을 직접 설계할 수 있다.\n",
    "  - **웹 기반으로 자신이 커스텀한 챗봇을 배포할 수 있는 역량을 갖춘다.**\n",
    "\n",
    "- **핵심 키워드**\n",
    "  - LLM Chatbot, LangChain, LCEL, Prompt, Streamlit, FAISS, RAG  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e797e",
   "metadata": {},
   "source": [
    "### 환경 설정 \n",
    "\n",
    "#### (1) 필수 라이브러리 설치 \n",
    "웹 페이지(UI) 를 직접 띄워야 하기 때문에, Google Colab보다는  \n",
    "본인 PC의 로컬 환경(VSCode, Terminal 등) 에서 실행하는 것을 권장합니다.\n",
    "\n",
    "먼저 실습에 필요한 주요 패키지를 설치합니다.\n",
    "터미널 또는 VSCode의 터미널 창에서 다음 명령어를 입력하세요.\n",
    "```bash\n",
    "pip install -U langchain langchain-openai streamlit faiss-cpu python-dotenv tiktoken\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc5d16",
   "metadata": {},
   "source": [
    "#### (2) 라이브러리별 역할 설명\n",
    "\n",
    "| 라이브러리                | 주요 역할        | 설명                                                                                                  |\n",
    "| -------------------- | ------------ | --------------------------------------------------------------------------------------------------- |\n",
    "| **langchain**        | 체인 프레임워크 핵심  | LLM(대형언어모델)을 중심으로 다양한 입력·출력·도구를 연결하는 프레임워크. `PromptTemplate`, `LLMChain`, `Retriever`, `LCEL` 등을 제공 |\n",
    "| **langchain-openai** | OpenAI 연동 모듈 | OpenAI의 GPT 모델(`ChatOpenAI`)을 LangChain 체인 안에서 바로 사용할 수 있도록 연결해줌                                    |\n",
    "| **streamlit**        | 웹 인터페이스 구현   | Python 코드 몇 줄로 대화형 웹 UI(챗봇 화면, 입력창, 출력창 등)를 만들 수 있는 간단한 프레임워크                                       |\n",
    "| **faiss-cpu**        | 벡터 검색 DB     | 문서 검색 기반 챗봇(RAG) 구현 시, 텍스트 임베딩을 벡터로 저장하고 유사한 내용을 빠르게 검색하는 역할                                        |\n",
    "| **python-dotenv**    | 환경변수 관리      | `.env` 파일에서 OpenAI API Key 등 민감한 정보를 안전하게 불러와 코드에 직접 노출하지 않게 해줌                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1d3dc",
   "metadata": {},
   "source": [
    "#### (3) OpenAI API 키 설정 (.env 파일, 보안 주의)\n",
    "\n",
    "**API란?**\n",
    "\n",
    "하나의 프로그램이 다른 프로그램이나 시스템과 **데이터를 주고받고 기능을 활용할 수 있도록 연결해주는 인터페이스**를 의미합니다.  \n",
    "즉, **서로 다른 소프트웨어 간의 다리 역할을 하는 통신 규칙**입니다.\n",
    "\n",
    "예를 들어, OpenAI의 **GPT API**는 사용자가 직접 모델을 설치하지 않아도  \n",
    "OpenAI 서버에 **질문(요청)** 을 보내고, 그 결과로 **답변(응답)** 을 받을 수 있게 해주는 API입니다.\n",
    "\n",
    "→ 쉽게 말해, **필요할 때마다 GPT에게 요청을 보내 답변을 받아오는 온라인 시스템**이라고 생각하시면 됩니다.\n",
    "\n",
    "※ 이 API는 **사용량 기반으로 과금되는 유료 서비스**이며, 웹에서 제공되는  \n",
    "ChatGPT의 무료·유료 플랜과는 **별도의 요금 체계**로 운영됩니다.\n",
    "\n",
    "\n",
    "※ 현재 사용 가능한 대표 GPT API 모델 일부 (2025년 10월 기준)\n",
    "\n",
    "| 모델 이름 | 설명 | 입력 가격 (per 1M tokens) | 출력 가격 (per 1M tokens) |\n",
    "|-----------|--------|---------------------------|----------------------------|\n",
    "| `gpt-3.5-turbo` | 초창기 모델. 현재 더 저렴하고 성능이 좋은 gpt-4.1-mini 모델 등이 사용 가능| **\\$0.5** | **\\$1.5** |\n",
    "| `gpt-4.1-mini` | 성능, 속도, 비용 측면에서 골고루 균형이 잡힌 모델| **\\$0.4** | **\\$1.6** |\n",
    "| `gpt-4.1-nano` | 성능을 조금 희생하고 속도와 비용 측면에서 최적화를 이룬 모델| **\\$0.1** | **\\$0.4** |\n",
    "| `o4-mini` | 빠르고 효과적인 추론에 최적화되어 있으며, 코딩 및 시각 작업에서 탁월한 성능을 발휘 | **\\$1.1** | **\\$4.4** |\n",
    "| `gpt-5-mini` | 빠르고 효율적인 비용이며 명확한 작업을 수행하는 모델 | **\\$0.25** | **\\$2.0** |\n",
    "| `gpt-5-nano` | GPT-5 모델을 빠르고 가장 효율적인 비용으로 활용할 수 있는 모델 | **\\$0.05** | **\\$0.4** |\n",
    "\n",
    "※ OpenAI는 지속적으로 모델명을 업데이트하고 있으므로, [공식 문서](https://platform.openai.com/docs/models)에서 최신 목록을 확인하는 것이 좋습니다.\n",
    "\n",
    "최신 요금은 OpenAI 공식 문서에서 확인하세요:  \n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f510169",
   "metadata": {},
   "source": [
    "**API Key란?**  \n",
    "GPT API를 사용하기 위해서는 OpenAI 서버가 요청을 보낸 사용자를 식별하고,  \n",
    "해당 사용자의 **이용량과 과금 내역을 추적**할 수 있어야 합니다.  \n",
    "이를 위해 OpenAI는 각 사용자에게 **고유한 인증 토큰(API Key)** 을 발급합니다.\n",
    "\n",
    "이 키는 **요금 청구의 기준이 되는 중요 정보**이며, 외부에 노출될 경우 **타인이 임의로  \n",
    "사용해 요금이 발생할 수 있으므로 철저한 보안 관리가 필요**합니다.\n",
    "\n",
    "\n",
    "**발급 절차**\n",
    "\n",
    "1. [https://platform.openai.com/](https://platform.openai.com/) 접속  \n",
    "2. 로그인 또는 계정 신규 생성  \n",
    "3. 우측 상단 프로필 아이콘 클릭 → `View API Keys` 선택  \n",
    "4. 결제 수단(신용카드 등) 등록  \n",
    "5. `+ Create new secret key` 클릭 → **발급된 Key를 복사해 안전한 위치에 보관**\n",
    "\n",
    "**발급된 Key는 한 번만 표시되므로**, 반드시 즉시 메모장이나 `.env` 파일 등에 백업해 두세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618136bf",
   "metadata": {},
   "source": [
    "실습 폴더 루트에 .env 파일을 생성하고, 강사가 안내한 OpenAI API Key를 아래 형식으로 저장합니다. \n",
    "```bash\n",
    "OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "``` \n",
    "\n",
    "#### ⚠️ 주의\n",
    ".env 파일은 절대 깃허브(GitHub)나 외부 저장소에 업로드하지 마세요.  \n",
    "사용은 `.env` 파일에 저장 후 `python-dotenv`를 이용해 불러오는 방식을 권장합니다.\n",
    "API Key가 노출되면 과금 위험이 있습니다.\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e6fec",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "5fd266a8-7311-4331-9fd2-1a4e014ebd5e"
    }
   },
   "source": [
    "##  2. LangChain 소개\n",
    "### 🔹 LangChain이란?\n",
    "\n",
    "<img src =\"image/LangChain.png\" width=\"500\">\n",
    "\n",
    "LangChain은 **대규모 언어모델(LLM)** 을 더 쉽게 활용할 수 있도록 도와주는 **프레임워크**입니다.  \n",
    "\n",
    "단순히 LLM에게 질문을 던지는 수준을 넘어서, **프롬프트 관리(prompt management)**,  \n",
    "**메모리(memory)**, **외부 도구(tool) 연동**, **문서 검색(RAG)** 등을 체계적으로 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c7fa9",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "d653c966-c5b2-45bc-8983-42665894ef6c"
    }
   },
   "source": [
    "예를 들어, 우리가 ChatGPT 같은 LLM을 사용할 때  \n",
    "단순히 모델 하나만으로 뛰어난 답변이 만들어지는 것은 아닙니다.  \n",
    "\n",
    "실제로는 **검색(Search)**, **분석(Analysis)**, **정보 정리(Summarization)** 등  \n",
    "여러 절차가 백그라운드에서 함께 작동해야 더 정확하고 풍부한 답변을 얻을 수 있습니다.  \n",
    "\n",
    "LangChain은 이러한 여러 단계를 **하나의 체계적인 구조로 묶어주는 프레임워크**입니다.  \n",
    "즉, LLM이 단독으로 모든 일을 처리하는 대신  \n",
    "“사용자 입력 → 관련 정보 검색 → 분석 및 요약 → 결과 생성”과 같은  \n",
    "복잡한 흐름을 하나의 **체인(chain)** 으로 자동화할 수 있습니다.  \n",
    "\n",
    "<img src=\"image/chatgpt.jpg\" width=\"600\">\n",
    "\n",
    "예를 들어, 사용자가 “3박 4일, 30만 원 예산, 맛집 중심의 서울 여행”을 요청하면,  \n",
    "LangChain은 내부적으로 다음과 같은 과정을 수행합니다.  \n",
    "\n",
    "> 입력 분석 → 예산과 일정 파악 → 추천 장소 검색 → 일정표 생성 → 자연스러운 문장으로 요약  \n",
    "\n",
    "이처럼 LangChain은 단순한 대화 요청을 넘어 **검색, 분석, 조합, 요약** 등의 단계를 유기적으로 연결해  \n",
    "LLM이 더 똑똑하게 작동하도록 돕습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd760d",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "39e28d56-89e6-4c0e-afc9-74f2fb45d10b"
    }
   },
   "source": [
    "또한 LangChain은 **LCEL (LangChain Expression Language)** 이라는 표현 언어를 통해  \n",
    "이러한 여러 단계(components)를 **직관적으로 연결**할 수 있게 해줍니다.  \n",
    "즉, “LLM → 요약기 → 출력 포맷터”처럼 **함수형 파이프라인**을 선언적으로 작성할 수 있어  \n",
    "확장성과 유지보수성이 뛰어납니다.\n",
    "\n",
    "LCEL 예시: \n",
    "```text\n",
    "사용자입력\n",
    "  | 분석기(요청에서 날짜·예산·키워드 추출)\n",
    "  | 검색기(분석 결과를 바탕으로 추천 장소 탐색)\n",
    "  | 일정생성기(추천 장소로 일자별 일정 구성)\n",
    "  | 요약기(자연스러운 문장으로 결과 정리)\n",
    "  | 출력\n",
    "```\n",
    "\n",
    "이처럼 LCEL은 각 단계를 함수처럼 “연결(pipe)”하여 데이터가 흘러가는 경로를 선언적으로 표현합니다.  \n",
    "복잡한 제어문 없이 “입력 → 처리 → 출력” 구조를 직관적으로 구성할 수 있어,\n",
    "체인 전체의 논리 흐름을 한눈에 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8ba0d",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "dcfee518-59cd-4dfe-a0ac-a2c1563ee7d0"
    }
   },
   "source": [
    "### 🔹 간단한 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f3bf0",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "ff5d9896-85ae-4b58-a254-481cd8eea233"
    }
   },
   "outputs": [],
   "source": [
    "# OpenAI API를 LangChain에서 쉽게 쓸 수 있도록 해주는 래퍼 모듈\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts  import PromptTemplate # 문자열 템플릿 기반으로 프롬프트를 자동 구성해주는 유틸\n",
    "from langchain_core.output_parsers import StrOutputParser # LLM의 응답 객체를 문자열로 변환해주는 파서 (출력 후처리 단계)\n",
    "from dotenv import load_dotenv # .env 파일의 환경변수를 자동으로 불러오기 위한 모듈\n",
    "load_dotenv()  # 실행 시 .env 파일을 찾아 변수들을 환경에 로드\n",
    "\n",
    "# LCEL 방식: 파이프라인 형태로 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = PromptTemplate.from_template(\"'{topic}' 주제에 대해 한 문장으로 설명해줘.\")\n",
    "output_str = StrOutputParser() # 결과 객체에서 텍스트(content)만 깔끔하게 추출해 문자열로 변환함\n",
    "\n",
    "# LCEL 표현 (프롬프트 → LLM → 출력)\n",
    "chain = prompt | llm | output_str\n",
    "\n",
    "# chain.invoke() 는 전체 파이프라인을 한 번 실행하는 메서드\n",
    "result = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa68bd",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "f6f14b93-69e6-458c-aed6-366dbbcec92a"
    }
   },
   "source": [
    "| 항목            | 설명                              |\n",
    "| ------------- | ------------------------------- |\n",
    "| **LangChain** | LLM을 활용하기 위한 파이썬 기반 프레임워크       |\n",
    "| **핵심 기능**     | 프롬프트 관리, 메모리, 도구 연동, 문서 검색(RAG) |\n",
    "| **LCEL**      | 체인 구성 과정을 간결하게 표현하는 언어          |\n",
    "| **활용 예시**     | 질문응답, 요약, 문서검색, 챗봇, 자동화된 지식시스템  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26d3a3",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "90a919b0-f776-4841-9032-e0228fe8a4f8"
    }
   },
   "source": [
    "LangChain은 LLM의 “두뇌”를 실무에서 “자동화 가능한 시스템”으로 바꿔주는 연결 프레임워크입니다.  \n",
    "단순한 모델 호출이 아니라, 프로세스를 설계하고 실행하는 엔진이라고 이해하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e9f12",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "6bc0b48a-583f-41b6-95de-d11a3083ed48"
    }
   },
   "source": [
    "## 3. 프롬프트(Prompt)\n",
    "\n",
    "**프롬프트(Prompt)** 는 LLM(대형언어모델)에게 “무엇을, 어떻게 해달라”고 **지시하는 문장**입니다.\n",
    "\n",
    "사람에게 질문할 때도 요청을 명확히 해야 원하는 답을 얻을 수 있듯,  LLM에게도 프롬프트가 곧 **“명령어이자 대화의 시작점”** 이 됩니다.  \n",
    "\n",
    "예를 들어,  \n",
    "> 1. “고양이에 대해 설명해줘.”  \n",
    "> 2. “고양이를 5살 아이가 이해할 수 있게 설명해줘.”  \n",
    "\n",
    "두 문장은 같은 주제지만 결과는 완전히 다릅니다.  \n",
    "👉 따라서 **프롬프트의 설계(=프롬프트 엔지니어링)** 가 답변의 질을 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24081896",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "d32d183a-d2dd-4c0f-a19c-629000527024"
    }
   },
   "source": [
    "#### LLM은 세 가지 종류의 프롬프트로 대화한다\n",
    "LLM은 단순히 한 문장만 보고 답하지 않습니다.  \n",
    "대화의 **맥락(context)** 을 유지하기 위해 다음 세 가지 프롬프트를 함께 사용합니다.\n",
    "\n",
    "| 종류 | 역할 | 예시 |\n",
    "|------|------|------|\n",
    "| **System Prompt** | 모델의 성격과 역할을 설정 | “너는 친절하고 이해심 많은 선생님이야.” |\n",
    "| **User Prompt** | 사용자의 실제 질문이나 요청 | “인공지능이 뭐야?” |\n",
    "| **Assistant Prompt** | 모델이 이전에 했던 응답 | “인공지능은 사람의 학습 능력을 모방한 기술이야.” |\n",
    "\n",
    "#### 왜 이렇게 구분할까?\n",
    "- **System Prompt** → 모델의 **기본 성격**을 결정합니다. (친절한 선생님 / 냉정한 분석가 등)  \n",
    "- **User Prompt** → 사용자가 **무엇을 알고 싶은지**를 알려줍니다.  \n",
    "- **Assistant Prompt** → 이전 대화 내용을 **기억하고 맥락을 유지**하게 도와줍니다.  \n",
    "\n",
    "이 세 가지를 함께 쓰면 모델은  \n",
    "> “지금 나는 친절한 선생님이고, 방금 ‘인공지능이 뭐냐’는 질문을 들었구나.”  \n",
    "라고 이해한 뒤 더 자연스럽고 일관된 답변을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a795d",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "e2c8e711-52e5-4954-814a-28e6f20bcb0d"
    }
   },
   "source": [
    "### 🔹 LangChain에서의 프롬프트: `PromptTemplate`\n",
    "LangChain에서는 프롬프트를 **하드코딩(직접 작성)** 하는 대신, **`PromptTemplate`** 클래스를 사용해 “템플릿” 형태로 관리합니다.\n",
    "\n",
    "#### 예시 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f57f6",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "f764be05-222f-4360-869c-c4ecc44a1d63"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts  import PromptTemplate\n",
    "\n",
    "# {topic} 부분에 사용자의 입력이 들어가도록 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"'{topic}'에 대해 초보자도 이해할 수 있게 한 문장으로 설명해줘.\")\n",
    "\n",
    "# 실제 실행 시:\n",
    "prompt.format(topic=\"LangChain\")\n",
    "# 결과 → \"'LangChain'에 대해 초보자도 이해할 수 있게 한 문장으로 설명해줘.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3575456",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "b50542bd-06c7-46c4-8529-fddc23efacf7"
    }
   },
   "source": [
    "#### 예시 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34e180",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "7fd41289-79e2-4755-b0f0-f7ac1560d633"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1.  모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# 2️. 프롬프트 구성\n",
    "# - System: 모델의 기본 역할(여행 전문가)\n",
    "# - User: 사용자의 첫 질문 (가을 여행지 추천)\n",
    "# - Assistant: 직전에 모델이 대답했던 내용 (AI의 응답)\n",
    "# - User: 현재 사용자의 새로운 질문 (부산 일정표 요청)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 친절한 여행 전문가야. 사용자의 취향에 맞게 국내 여행지를 추천하고, 일정도 구체적으로 제안해줘.\"),\n",
    "    (\"user\", \"가을에 2박 3일로 갈만한 국내 여행지를 추천해줘.\"),\n",
    "    (\"assistant\", \"부산, 여수, 속초가 좋을 것 같아요! 각각 바다와 음식, 분위기가 다르니 선택에 따라 즐길 거리가 달라요.\"),\n",
    "    (\"user\", \"그럼 이번엔 부산으로 2박 3일 일정표를 만들어줘.\")\n",
    "])\n",
    "\n",
    "# 3️. 출력 파서 (응답 텍스트만 추출)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4️. LCEL 파이프라인 구성 (Prompt → LLM → Parser)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 5️. 실행\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074addf",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "d85e107c-26c9-4a3f-9d04-803c3c711ecb"
    }
   },
   "source": [
    "#### 예시 3. 퓨샷 러닝(Few-shot Learning)\n",
    "\n",
    "**Few-shot Learning**은 LLM에게 **“예시 몇 개를 직접 보여줘서 학습 방향을 잡아주는 방법”** 입니다.\n",
    "\n",
    "AI에게 “이렇게 대답해!”라고 명령하는 것만으로는 부족할 때,  \n",
    "실제 **입출력 예시를 몇 개 제시해주면** 모델이 그 **패턴과 말투, 구조**를 빠르게 학습해 유사한 답변을 생성합니다.\n",
    "\n",
    "예를 들어  \n",
    "> “백종원 대표의 충청도 사투리로 요리 팁을 말해줘.”  \n",
    "라고만 하면 AI는 사투리의 뉘앙스를 정확히 몰라 평범한 답변을 할 수 있습니다.  \n",
    "\n",
    "하지만 아래처럼 실제 대화 예시를 1~2개 보여주면  \n",
    "> “아~ 이런 식으로 말하면 되는구나!”  \n",
    "하고 패턴을 스스로 학습해 자연스러운 말투를 재현합니다.\n",
    "\n",
    "**퓨샷 러닝의 장점**\n",
    "\n",
    "| 장점 | 설명 |\n",
    "|------|------|\n",
    "|**말투·스타일 학습 가능** | 특정 인물(예: 백종원, 아이유, 선생님 등)의 말투나 표현방식 재현 |\n",
    "|**작업 포맷 유지** | 요약, 번역, 해설 등 특정 형식을 예시로 제시하면 그 구조를 그대로 따름 |\n",
    "|**추가 학습(파인튜닝) 없이 가능** | 단순히 프롬프트 안에 예시 몇 개 넣는 것만으로도 “즉석 학습” 효과 |\n",
    "|**창의적 제어** | “~처럼 말해줘”, “이런 형식으로 답해줘” 등 자유로운 응용 가능 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41391e",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "b39f21b7-f98d-494e-93a9-b1b4f8be49b5"
    }
   },
   "outputs": [],
   "source": [
    "### 코드 예시: 백종원 말투 흉내내기 (Few-shot Learning)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# temperature를 약간 높이면(0.8) 표현이 더 자유롭고 창의적이게 됨\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "\n",
    "# - System: 모델의 기본 역할 지정\n",
    "# - 예시 대화(few-shot examples): 실제 말투 패턴을 보여줌\n",
    "# - User: 새로운 질문\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 요리 전문가이자, 백종원 대표의 말투를 자연스럽게 흉내내는 AI야. 충청도 사투리와 친근한 말투로 대답해.\"),\n",
    "    \n",
    "    # [예시 1] — 사용자가 물어봤고, 백종원 말투로 답변한 사례\n",
    "    (\"user\", \"라면 맛있게 끓이는 비법 알려주세요.\"),\n",
    "    (\"assistant\", \"그라믄 말이여~ 라면은 물 끓일 때 스프를 같이 넣는 게 포인트여. 그래야 국물이 진~해진다잉.\"),\n",
    "    \n",
    "    # [예시 2] — 또 다른 백종원식 대답 예시\n",
    "    (\"user\", \"김치찌개 맛있게 끓이는 법은요?\"),\n",
    "    (\"assistant\", \"김치찌개는 묵은지 써야혀~ 그래야 깊은 맛이 나. 돼지고기 좀 넣어주면 금상첨화여잉.\"),\n",
    "    \n",
    "    # [새로운 요청] — 실행 시 전달될 질문 변수. 이제 모델이 같은 말투로 이어서 대답해야 함\n",
    "    (\"user\", \"{question}\")])\n",
    "\n",
    "# 출력 파서 (응답 텍스트만 추출)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL 파이프라인 구성 (Prompt → LLM → Parser)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 실행 — invoke()에 직접 새로운 질문 전달\n",
    "result = chain.invoke({\"question\": \"김치전은 어떻게 만들면 맛있어요?\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac65ec0",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "fdd1b71c-db40-4c5c-83ad-40531fa3694a"
    }
   },
   "source": [
    "##  4. Q&A 챗봇 실습 (LCEL)\n",
    "\n",
    "앞서 배운 **프롬프트 설계**와 **LCEL 파이프라인 구성법**을 직접 활용하여  \n",
    "“질문 → 답변” 형태의 **Q&A 챗봇**을 완성해보세요.\n",
    "\n",
    "단순히 모델을 호출하는 것이 아니라, **프롬프트 구조를 설계**하고 **출력 파서를 연결**하며  \n",
    " **LCEL 문법(| 연산자)** 을 이용해 **체인형 파이프라인**을 구성해야 합니다.\n",
    "\n",
    "##### 실습 시나리오\n",
    "> 당신은 **AI Q&A 챗봇 개발자**입니다.  \n",
    "> 사용자가 “기술 관련 질문”을 하면, 챗봇은 **요약 + 예시 + 비유 설명**을 포함한 답변을 하도록 만들어야 합니다.  \n",
    "> (예: “REST API란?”, “클라우드 컴퓨팅이 뭐야?”, “RAG는 어떤 구조야?” 등)\n",
    "\n",
    "즉, 단순한 정의를 넘어 “무엇인지 + 왜 중요한지 + 예시” 를 한 번에 설명하는 챗봇을 설계하세요.\n",
    "\n",
    "**과제 요구사항**\n",
    "\n",
    "1. **`ChatPromptTemplate`** 을 사용해 아래 구조의 프롬프트를 직접 설계하세요.\n",
    "   - `system`: 챗봇의 역할 정의  \n",
    "     (예: “너는 기술 개념을 쉬운 비유와 예시를 섞어서 설명하는 AI 선생님이야.”)\n",
    "   - `user`: 사용자의 질문을 `{question}` 변수로 전달\n",
    "\n",
    "2. **`ChatOpenAI`** 모델을 연결하세요.  \n",
    "   - 모델: `\"gpt-4o-mini\"`  \n",
    "   - `temperature=0.7` (조금 창의적인 답변 유도)\n",
    "\n",
    "3. **`StrOutputParser`** 를 사용하여 결과를 문자열로 출력하세요.\n",
    "\n",
    "4. **LCEL 문법**으로 체인을 완성하세요.  \n",
    "   예:  \n",
    "   ```python\n",
    "   chain = prompt | llm | parser\n",
    "   ```\n",
    "5. 실행해보고 답변이 지나치게 딱딱하거나 비유나 예시가 부족하면 Prompt를 직접 수정하면서 성능을 조정해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2b98b",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "9ce38f66-76a8-44f9-a7e1-01de3b8227d9"
    }
   },
   "source": [
    "**질문 답변 예시:**  \n",
    "\n",
    "질문 : REST API란?  \n",
    "\n",
    "답변 :   \n",
    "\n",
    "① **정의**: REST API(Representational State Transfer Application Programming Interface)는 웹 상에서 서로 다른 프로그램이나 시스템이 데이터를 주고받을 수 있도록 하는 규칙과 방법을 정의한 인터페이스입니다. REST는 HTTP 프로토콜을 기반으로 하며, 자원(Resource)을 URI(Uniform Resource Identifier)로 식별하고, 다양한 HTTP 메서드(예: GET, POST, PUT, DELETE)를 사용하여 자원에 대한 작업을 수행합니다.\n",
    "\n",
    "② **이유(중요성)**: REST API는 서로 다른 시스템 간의 통신을 쉽고 효율적으로 만들어줍니다. 이를 통해 개발자들은 복잡한 시스템을 간단하게 연결하고, 데이터를 주고받을 수 있습니다. REST API는 웹 기반 애플리케이션에서 표준으로 자리잡고 있어, 다양한 서비스를 통합하고 확장하는 데 매우 중요합니다.\n",
    "\n",
    "③ **쉬운 예시**: REST API를 설명하기 위해 레스토랑을 비유로 들어볼 수 있습니다. 레스토랑의 메뉴판은 고객이 어떤 음식을 주문할 수 있는지를 보여주는 역할을 합니다. 고객(클라이언트)은 메뉴판(URI)을 보고 원하는 음식을 선택하고, 주문(HTTP 메서드)을 합니다. 주방(서버)은 고객의 주문을 받아 음식을 준비하고, 다시 고객에게 서빙합니다. 이 과정에서 메뉴판은 고객과 주방 간의 소통을 원활하게 해주는 역할을 하며, REST API는 시스템 간의 소통을 원활하게 해주는 역할을 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d81c3d",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "77604b46-fca7-4c7e-b11f-0def18ed7848"
    }
   },
   "outputs": [],
   "source": [
    "### 여기에 과제 요구 사항을 작성하세요\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b2c11",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "60e8ee8a-cc08-45d3-922d-54bde8eacbc7"
    }
   },
   "source": [
    "<details>\n",
    "<summary>정답 보기</summary>\n",
    "\n",
    "```python \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7,  max_tokens=200)\n",
    "\n",
    "# 프롬프트 설계\n",
    "# system: 챗봇의 역할 정의\n",
    "# user: {question} 변수를 통해 질문 전달\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"너는 기술 개념을 알기 쉽게 설명하는 AI 선생님이야. \"\n",
    "     \"답변은 반드시 ① 정의 ② 이유(중요성) ③ 쉬운 예시를 포함해야 해. \"\n",
    "     \"필요하다면 비유를 사용해도 좋아.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 출력 파서\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 실행 테스트\n",
    "question = \"REST API란?\"\n",
    "answer = chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"🧠 질문: {question}\\n\")\n",
    "print(f\"💬 답변:\\n{answer}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b202d36",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "5c109850-714d-438e-95d2-18d73bb1c30f"
    }
   },
   "source": [
    "##  5. 대화형 챗봇 (메모리)\n",
    "\n",
    "### 🔹 Conversation Memory란?\n",
    "> **Conversation Memory(대화 메모리)** 는 LLM이 **이전 대화 내용을 기억하고 이어서 대답**할 수 있도록 하는 기능입니다.\n",
    "\n",
    "일반적인 LLM은 한 번의 요청(prompt)만 처리하고 끝나기 때문에,  \n",
    "사용자가 “아까 말한 여행지 일정 다시 알려줘.”처럼 과거 대화를 참조하면 맥락을 잃어버립니다.  \n",
    "→ 이때 필요한 것이 바로 **Memory(기억 기능)** 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb2d23",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "228827c6-01b3-4984-b0d7-40a2950b77ae"
    }
   },
   "source": [
    "### 🔹 왜 Memory가 필요할까?\n",
    "\n",
    "| 상황 | Memory 없을 때 | Memory 있을 때 |\n",
    "|------|----------------|----------------|\n",
    "| 사용자: “부산 여행지 추천해줘.”<br>→ 모델이 답변함 | 다음 질문: “그럼 거기 근처 맛집은?”<br>→ 모델이 “어디 여행 말씀인가요?” | “부산 근처엔 회센터가 많고… 홍합탕집이 유명해요!” |\n",
    "| 사용자: “어제 추천해준 책 제목 다시 말해줘.” | “어떤 책을 말씀하시는 건가요?” | “어제 말씀드린 건 『데미안』이에요.” |\n",
    "\n",
    "👉 메모리를 활용하면,  \n",
    "모델이 “과거 대화의 맥락(Context)”을 유지한 채로  \n",
    "**자연스러운 멀티턴(Multi-turn) 대화형 챗봇**을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668daf9",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "5b1a2d5d-6140-46d2-ba1a-8cfee2c88b55"
    }
   },
   "source": [
    "### 🔹 LangChain에서의 Memory 개념\n",
    "\n",
    "LangChain은 다양한 형태의 “기억 클래스(memory classes)”를 제공합니다.  \n",
    "대표적으로 아래 세 가지를 이해하면 충분합니다 👇\n",
    "\n",
    "| Memory 클래스 | 설명 | 특징 |\n",
    "|----------------|------|------|\n",
    "| **`ConversationBufferMemory`** | 단순히 대화 전체를 계속 저장 | 가장 기본적이고 직관적 |\n",
    "| **`ConversationBufferWindowMemory`** | 최근 N개의 대화만 저장 | “단기 기억” 형태로, 긴 대화를 효율적으로 유지 |\n",
    "| **`ConversationSummaryMemory`** | 이전 대화를 LLM이 요약해서 저장 | 긴 대화를 “핵심 요약본”으로 관리, 장기 기억에 적합 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87974665",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "346e6e3a-c301-4c81-acc4-956ddde7046d"
    }
   },
   "source": [
    "### 🔹 프롬프트에 직접 넣는 방식 vs Memory 클래스 사용 비교\n",
    "\n",
    "| 비교 항목 | 프롬프트에 직접 삽입 | Memory 클래스 사용 |\n",
    "|------------|----------------------|---------------------|\n",
    "| 코드 복잡도 | ❌ 대화마다 프롬프트를 새로 생성해야 함 | ✅ LangChain이 자동으로 이전 대화 삽입 |\n",
    "| 확장성 | ❌ 대화가 길어지면 토큰 초과 문제 발생 | ✅ 오래된 대화는 요약/제한 가능 |\n",
    "| 유지보수 | ⚠️ 과거 대화 삽입 위치만 교체하면 되지만, 요약·토큰 제어를 직접 구현해야 함 | ✅ 메모리 객체만 관리하면 됨 |\n",
    "| 현실성 | ❌ “일회용 챗봇”에만 적합 | ✅ “지속형 대화형 챗봇” 구현 가능 |\n",
    "\n",
    "즉, **Memory는 LLM이 “대화 히스토리를 스스로 관리”하도록 하는 스마트한 도우미 클래스**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb776f41",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "da75956c-6851-490b-82ae-1657be05ce31"
    }
   },
   "source": [
    "### 🔹 Memory 작동 원리\n",
    "1. 사용자가 LLM에 **질문(입력)** 을 보냅니다.  \n",
    "\n",
    "2. LLM이 **답변**을 생성하면, 이 대화 내용이 자동으로 **Memory에 저장**됩니다.  \n",
    "3. 사용자가 다음 질문을 입력하면,  \n",
    "   LangChain이 **이전 대화 기록을 자동으로 프롬프트에 포함시켜** LLM에 전달합니다.  \n",
    "4. LLM은 이 정보를 바탕으로 **맥락을 이해하고 연속적인 대화**를 생성합니다.\n",
    "\n",
    "→ 즉, 사용자는 별도로 과거 대화를 넣을 필요 없이, LangChain이 **“대화 기록 → 프롬프트 삽입 → LLM 호출”**  \n",
    "과정을 자동으로 처리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e415f",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "079f05f1-a21f-4222-b0ec-464d4aeb3e25"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 코드 예시 1. 기본 Buffer Memory (대화 전체 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ea2c8",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "4a72dc85-ebf9-423e-b1fd-059509c72817"
    }
   },
   "outputs": [],
   "source": [
    "### 코드 예시 ① 기본 Buffer Memory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 메모리 객체 생성 (대화 전체를 버퍼 형태로 저장)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 프롬프트 템플릿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 여행 전문가야. 사용자의 질문에 친절하게 답해줘.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 이전 대화 삽입 위치\n",
    "    (\"human\", \"{input}\")                           # 사용자 질문\n",
    "]) \n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = prompt | llm \n",
    "\n",
    "# 5️⃣ 연속 대화 시뮬레이션\n",
    "inputs = [\"부산 여행지 추천해줘.\", \"그럼 그 근처 맛집은 어디야?\"]\n",
    "\n",
    "for user_input in inputs:\n",
    "    # 메모리 불러오기\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    # 실행\n",
    "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    # 결과 출력\n",
    "    print(f\"\\n사용자: {user_input}\\n 응답: {result.content}\")\n",
    "    # 메모리에 저장\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": result.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c1755",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "94a3f7f3-fcd1-4b14-b0c3-eb6e13f14814"
    }
   },
   "source": [
    "모델은 앞선 “부산 여행” 대화를 기억하고 자연스럽게 이어서 대답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b50356",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "be45b2d9-ccf0-4d18-81fc-97f8f5e6e682"
    }
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a89041",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "d4ac988e-4458-4a3c-92ec-5df0e1afb1b9"
    }
   },
   "source": [
    "### 코드 예시 2. 최근 대화만 유지 (Window Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6e21e",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "783c5047-7723-49d4-9f6b-573a473095c1"
    }
   },
   "outputs": [],
   "source": [
    "### 코드 예시 ① 기본 Buffer Memory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 메모리 객체 생성 (대화 전체를 버퍼 형태로 저장)\n",
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "# 프롬프트 템플릿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 여행 전문가야. 사용자의 질문에 친절하게 답해줘.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 이전 대화 삽입 위치\n",
    "    (\"human\", \"{input}\")                           # 사용자 질문\n",
    "]) \n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = prompt | llm \n",
    "\n",
    "# 5️⃣ 연속 대화 시뮬레이션\n",
    "inputs = [\"부산 여행지 추천해줘.\", \"대한 민국의 수도는 어디야?\", \"서울의 인구는 몇명이야?\", \"내가 아까 추천해달라고 한 어행지는 어디야?\" ]\n",
    "\n",
    "for user_input in inputs:\n",
    "    # 메모리 불러오기\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    # 실행\n",
    "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    # 결과 출력\n",
    "    print(f\"\\n사용자: {user_input}\\n 응답: {result.content}\")\n",
    "    # 메모리에 저장\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": result.content})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d3fde",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "947088c4-22f9-490a-8ff3-115770b979d7"
    }
   },
   "source": [
    "이 방식은 단기 기억처럼 작동합니다.  \n",
    "오래된 대화는 자동으로 지워져 토큰 낭비를 줄이고 효율성을 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372ef50",
   "metadata": {},
   "source": [
    "### 코드 예시 3. 이전 대화를 요약해서 저장 (ConversationSummaryMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332aa80",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "aabfcc5e-77cf-4777-af3b-e312bad59ae2"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# 요약형 메모리 생성 — LLM이 과거 대화를 자동 요약하여 저장\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 여행 플래너야. 사용자의 요구에 따라 가족 여행 일정을 제안해줘.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 요약된 과거 대화 자동 삽입\n",
    "    (\"human\", \"{input}\")                           # 현재 질문\n",
    "])\n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = prompt | llm\n",
    "\n",
    "# 연속 대화 시뮬레이션\n",
    "inputs = [\n",
    "    \"이번 주말에 가족 여행지 추천해줘.\",\n",
    "    \"지난번에 추천한 곳 중에 아이들이 놀기 좋은 곳은 어디였지?\",\n",
    "    \"그럼 거기 일정표를 하루만 짜줘.\"\n",
    "]\n",
    "\n",
    "for user_input in inputs:\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    print(f\"\\n👤 사용자: {user_input}\\n🤖 응답: {result.content}\")\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12ef67",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "5b1e8332-d7e8-41c0-886c-b8256734b7ff"
    }
   },
   "outputs": [],
   "source": [
    "# 현재까지의 요약본 확인\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3324e",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "a6891393-447c-4e7e-9f46-79590cf1d99d"
    }
   },
   "source": [
    "이 방식은 LLM이 이전 대화를 스스로 요약하여 “장기 기억” 처럼 보관합니다.  \n",
    "수백 문장의 대화도 핵심 내용만 남기기 때문에 효율적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b812292",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "ed5c30bf-a5f0-4e58-9236-4793ca6457a1"
    }
   },
   "source": [
    "**핵심 정리**\n",
    "| Memory 타입 | 특징 | 사용 목적 |\n",
    "|--------------|--------|-------------|\n",
    "| **BufferMemory** | 모든 대화를 그대로 저장 | 짧은 대화, 디버깅용 |\n",
    "| **WindowMemory** | 최근 N개의 대화만 유지 | 실시간 채팅, 효율성 |\n",
    "| **SummaryMemory** | 이전 대화를 요약해서 유지 | 장기 기억, 맥락 유지형 챗봇 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0588e",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "2ba67127-fa73-408c-af86-dbc17b712a91"
    }
   },
   "source": [
    "🔹 실습 : 아래 조건을 만족하는 “나만의 대화형 여행 상담 챗봇”을 만들어보세요.\n",
    "\n",
    "당신은 AI 여행 비서 트래블GPT 입니다.  \n",
    "고객이 여러 도시를 순서대로 여행하면서 맞춤 일정·숙소·음식을 요청할 때,  \n",
    "이전 대화 내용을 기억하며 연결된 제안을 해주는 지능형 여행 어시스턴트를 구현하세요.  \n",
    "\n",
    "1. ChatOpenAI(model=\"gpt-4o-mini\") 사용\n",
    "2. ConversationSummaryMemory로 장기 대화 기억 구현\n",
    "3. 답변에 반드시 \"이전 여행 내용을 바탕으로 추천드리면...\" 이라는 문구 포함\n",
    "  - 예시 : 이전 여행 내용을 바탕으로 추천드리면, 여수에서는 해상케이블카와 낭만포차거리를 꼭 가보세요.\n",
    "4. 대화 시나리오:\n",
    "  - 사용자가 “부산 → 여수 → 강릉” 순으로 도시를 이동\n",
    "  - 챗봇은 이전 도시에서 한 활동을 기억하고 “연결된 여행 루트”나 “테마별 추천(가족/커플/힐링)”을 제안할 것\n",
    "\n",
    "예시 시나리오\n",
    "```bash\n",
    "사용자: 이번 주말엔 부산 갈 건데 가족 여행지 좀 추천해줘.\n",
    "AI: 부산의 해운대, 아쿠아리움이 가족 단위로 인기예요!\n",
    "\n",
    "사용자: 이번엔 여수로 가볼까?\n",
    "AI: 이전 여행 내용을 바탕으로 추천드리면, 부산의 해변 감성에 이어 여수에서는 바다 전망 케이블카와 낭만포차를 즐기세요.\n",
    "\n",
    "사용자: 그럼 마지막은 강릉이 좋을까?\n",
    "AI: 이전 여행 내용을 바탕으로 추천드리면, 강릉에서는 여수보다 조용한 힐링 카페 거리와 바다 일출 코스를 권합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14310e36",
   "metadata": {
    "cell_watermark": {
     "date": "2025-10-28",
     "hash": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
     "uuid": "0069e4be-432c-4734-b670-c740f30be739"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 모델 선언\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# 메모리 생성 — 이전 대화를 요약하며 장기 기억\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"너는 여행 비서 트래블GPT야. \"\n",
    "     \"사용자의 여행 루트를 기억하고, 이전 여행 내용을 바탕으로 다음 도시를 추천해줘. \"\n",
    "     \"답변에는 반드시 '이전 여행 내용을 바탕으로 추천드리면,' 이라는 문구를 포함해야 해.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 과거 대화 자동 삽입\n",
    "    (\"human\", \"{input}\")                            # 현재 사용자 입력\n",
    "])\n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = prompt | llm\n",
    "\n",
    "# 대화 시나리오\n",
    "inputs = [\n",
    "    \"이번 주말엔 부산 갈 건데 가족 여행지 좀 추천해줘.\",\n",
    "    \"이번엔 여수로 가볼까?\",\n",
    "    \"그럼 마지막은 강릉이 좋을까?\"\n",
    "]\n",
    "\n",
    "# 연속 대화 시뮬레이션\n",
    "for user_input in inputs:\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    print(f\"\\n 사용자: {user_input}\\n 트래블GPT: {result.content}\")\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": result.content})\n",
    "\n",
    "# 대화 요약 확인 (선택)\n",
    "print(\"\\n 요약된 Memory Buffer:\")\n",
    "print(memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
  "filename": "MeydvOywqC5pcHluYg==",
  "inserted_date": "2025-10-28",
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
