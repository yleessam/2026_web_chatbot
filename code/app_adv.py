import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ì±—ë´‡",
    page_icon="ğŸ’¬",
    layout="wide"
)

st.title("ğŸ’¬ AI ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    model = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"],
        index=1
    )
    
    # ì˜¨ë„ ì„¤ì •
    temperature = st.slider(
        "ì°½ì˜ì„± (Temperature)",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    st.subheader("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    system_prompt = st.text_area(
        "AIì˜ ì—­í• ê³¼ ì„±ê²©ì„ ì •ì˜í•˜ì„¸ìš”",
        value="ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
        height=150
    )
    
    st.divider()
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    # í†µê³„ í‘œì‹œ
    if "messages" in st.session_state:
        msg_count = len([m for m in st.session_state.messages if m["role"] == "user"])
        st.metric("ëŒ€í™” íšŸìˆ˜", msg_count)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages_with_system = [
            {"role": "system", "content": system_prompt}
        ] + st.session_state.messages
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        try:
            stream = client.chat.completions.create(
                model=model,
                messages=messages_with_system,
                temperature=temperature,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            message_placeholder.markdown(full_response)
    
    # AI ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})